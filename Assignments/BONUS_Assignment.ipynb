{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582d54ad-3ec0-474c-a51d-cbb0a5e7f520",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/marekpiotradamczyk/ml_uwr_23/blob/main/Assignments/BONUS_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22903a-bed2-4ac5-8d8d-f5ae5a66682e",
   "metadata": {},
   "source": [
    "# BONUS Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43d5cb-0639-4b7b-8fd5-3b70f8ad07ea",
   "metadata": {},
   "source": [
    "**Submission deadline:**\n",
    "* **12.2.24**\n",
    "\n",
    "**Points: 5 + 5 + [0, 1,...,10] BONUS points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a21e1-3452-4063-b87c-cae2a7a7e5bd",
   "metadata": {},
   "source": [
    "In this lecture https://github.com/marekpiotradamczyk/ml_uwr_23/blob/master/Lectures/10-neural_networks.ipynb you've seen a simple generative model for MNIST digits. The outline was as follows:\n",
    "1. Take from MNIST only images with digit 1, call this dataset $X^1_{mnist}$;\n",
    "2. Compute 1D t-SNE embeddings which you can view as a function $\\phi: \\mathbb{R}^{784} \\mapsto \\mathbb{R}$;\n",
    "3. Learn with a simple one hidden layer neural network a function inverse to $\\phi$;\n",
    "4. Now generate a random point $x\\in \\mathbb{R}$ which is reasonably close enough to the 1D embedding points, i.e., $\\phi(X^1_{mnist})$, calculate $\\phi^{-1}(x)$. Since $\\phi^{-1}(x) \\in \\mathbb{R}^{784}$, you can view $\\phi^{-1}(x)$ as an artificially created image with digit 1.\n",
    "5. We could see that the artificially generated images on one side nicely capture the high level features of the digit 1 from the mnist dataset. On the other hand however their quality wasn't as sharp as in the original MNIST. And it was confirmed by having a `k-NN(k=3)` discriminator, which had 95% accuracy in distinguishing between real and fake images.\n",
    "\n",
    "Remark: The random points in `4.` cannot be equal to the embedding of the actual MNIST images. Otherwise we would have reconstructions of the images and not artificial new ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5c5fd-1fba-4754-a4ee-eddf5f1c3498",
   "metadata": {},
   "source": [
    "# Task 1 [5p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb6c84-8ec7-491f-8e76-211722b7b040",
   "metadata": {},
   "source": [
    "Do very much the same, but\n",
    "1. Take from MNIST only images with digit 1, call this dataset $X^1_{mnist}$;\n",
    "2. Compute t-SNE embeddings but now with `n_components=3`\n",
    "   1. Scatterplot in 3D the obtained cloud of points\n",
    "4. Learn with a neural network a function inverse to $\\phi$; you can use as many layers as you want, but you cannot use convolutional layers;\n",
    "5. Generate randomly points from the 3D space of the embedding, and calculate $\\phi^{-1}$ on these points, which will give you a new set of images\n",
    "6. Assess the quality of the generation with a `k-NN(k=3)` discriminator. You should get accuracy less than $90\\%$ to consider the task solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750465c-1c14-4110-b787-8879f2495db4",
   "metadata": {},
   "source": [
    "**Remark about point 4** \n",
    "\n",
    "After plotting the 3D scatter plot in task `2.A.` you will see that the shape is not a perfect circle nor any easily definable manifold. So how one can draw random points from such a manifold boundary? \n",
    "\n",
    "There are many ways, but the simplest way would be as follows. First, calculate for each 3D embedded points of MNIST the distance to the closest neighbour. Now take the average such distance over all 3D embedded points, call it $\\varepsilon$.\n",
    "Once you calculate $\\varepsilon$ you can generate random points two ways.\n",
    "First:\n",
    "1. randomly pick an mnist digit $x$;\n",
    "2. look at its t-SNE embedding $\\phi(x) \\in \\mathbb{R}^3$;\n",
    "3. generate a random point from $\\mathcal{N}\\left(\\phi(x), \\varepsilon\\cdot \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\right)$, i.e., a point from a 3D Gaussian centered at $\\phi(x)$\n",
    "4. repeat couple of thousand times to get appropriate number of random points\n",
    "\n",
    "Second:\n",
    "1. the cloud of points should be roughly enclosed in a $[-20,20]\\times [-20,20] \\times [-20, 20] \\subset \\mathbb{R}^3$ cube\n",
    "2. generate randomly a point $x_{random}$ from this cube\n",
    "3. calculate the distance to the closest point in the cloud (u can use `from sklearn.neighbors import NearestNeighbors` for example)\n",
    "4. if the distance is less than $\\varepsilon$, then take it to the set of random points, and otherwise discard it\n",
    "5. repeat enough many thousand of times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3183f18-1174-49d9-bd81-db7df87d083c",
   "metadata": {},
   "source": [
    "# Task 2 [5p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a51fc-e3e6-4fbd-961b-6ee4cbd285f1",
   "metadata": {},
   "source": [
    "Do the same as above, but don't use a neural network to learn $\\phi^{-1}$. Try using gradient boosting methods or polynomial regression method, or use non-trivial features for the task. You don't have to write algorithms by hand if you can find a proper implementation from some python package. However please note that the code of gradient boosting from lecture https://github.com/marekpiotradamczyk/ml_uwr_23/blob/master/Lectures/11-boosting.ipynb is relatively simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68287657-0a41-426a-9e0c-371d4f418ff1",
   "metadata": {},
   "source": [
    "# Competition [1-10p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26369e7-079e-477f-a8a1-c0ecfd4f3bf8",
   "metadata": {},
   "source": [
    "Every submitted solution to Task 1 will be ranked among other students' submissions. The smaller the accuracy of the `k-NN(k=3)` the higher the ranking. Author of the first place submission will score 10 points, author of the second place submission will score 9 points, and so on.\n",
    "\n",
    "**Remarks** in Task 1 you just need to use 3d t-SNE embeddings in order to score 5 points for the task. However in the competition we may allow embeddings of bigger dimension, but not too big. Let's say that the dimension of embedded space cannot be greater than 12. `sklearn` implementation of t-SNE doesn't allow for `n_components` bigger than 3, but feel free to use absolutely any method of embedding MNIST into $\\mathbb{R}^{12}$ that you can find in some python package, or write it yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9730b66-14ec-4d25-9e31-af4c508a0448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_uwr_kernel",
   "language": "python",
   "name": "ml_uwr_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
